// cfi_optimizer_wrapper.cpp
#include <stdint.h>
#include <stdio.h>

#ifndef d0ebdb30_7057_4381_8bec_14222d7952c4
#define d0ebdb30_7057_4381_8bec_14222d7952c4

#include <immintrin.h>
#include <stdbool.h>
#include <stdint.h>

#ifndef HM_DEFAULT_N_GROUPS
#define HM_DEFAULT_N_GROUPS (1)
#endif

#ifndef HM_LOAD_FACTOR
#define HM_LOAD_FACTOR (0.75)
#endif

#ifndef HM_RESIZE_FACTOR
#define HM_RESIZE_FACTOR (2)
#endif

#define HM_GROUP_SIZE (16)
#define HM_CONTROL_SIZE (16)

typedef struct
{
    size_t class_id; // a type identifier generated by the clang/llvm;
    int vptr;        // Using int to store pointer values for better memory efficiency;
    int data;        // extra information
} hm_keyv_t;

typedef size_t hm_data_t;
typedef __m128i hm_control_t;
typedef int8_t hm_metadata_t;

typedef enum // Usage types for this hashmap
{
    HM_TYPE_VERIFY = 0, // Only key-value pairs
    HM_TYPE_RECORD = 1, // with additional data
} hm_usage_t;

typedef enum
{
    HM_EMPTY1B = 0b10000000,
    HM_EMPTY8B = 0x8080808080808080UL,
    HM_DELETED = 0b11111111
} hm_ctrl_e;

typedef struct
{
    hm_metadata_t meta;
    int pos;
} hm_hash_t;

typedef struct // Each group of swiss-table contains 16 slots.
{
    hm_control_t _ctrl;
    hm_keyv_t keyv[HM_CONTROL_SIZE];
    hm_hash_t hash[HM_CONTROL_SIZE];
} hm_group_t;

#define MAP_EVICT_MIN_COUNT 10 // Minimum number of entries to evict during reduction
#define MAP_MIGRATE_MIN_FREQ 4 // Minimum value to migrate from record_map

typedef struct
{
    hm_usage_t cache_type;
    int oldest_generation; // The generation of the oldest entries;
    int newest_generation; // Used for eviction entries in verify_cache;
    int eviction_min_freq; // Used for eviction entries in record_cache;
} __attribute__((aligned(8))) hm_cache_t;

// A swiss-table is a hashmap with a fixed number of groups,
typedef struct
{
    hm_cache_t metainfo;

    int items, size;
    int n_groups, sentinel;
    hm_group_t groups[0] __attribute__((aligned(32)));
} hm_map_t;

//------------------------Begin: Model-level data structures------------------------------
#define PAGE_SIZE 4096 // Assuming a page size of 4096 bytes
// Helper macro to round up to system page size
#define ROUND_TO_PAGESIZE(size) ((size + PAGE_SIZE - 1) & ~(PAGE_SIZE - 1))
// Helper macro to round down to the nearest power of two
#define ROUND_DOWN_TO_POW2(x) ((size_t)1 << (63 - __builtin_clzl(x)))

// Common layout for a recording and verification hashmap caches
typedef struct
{
    hm_map_t hashmap;                                      // The hashmap instance
    hm_group_t map_groups[1] __attribute__((aligned(32))); // Adjacent group array
} hm_cache_layout_t;

// A hashmap for recording VCALL signatures
#define RECORD_GROUP_NUM 10 // 10 groups for recording, ~1 pages
typedef struct              // Inherits from hm_cache_layout_t
{
    hm_map_t hashmap;                                                     // The hashmap instance
    hm_group_t map_groups[RECORD_GROUP_NUM] __attribute__((aligned(32))); // Adjacent group array
} __attribute__((aligned(PAGE_SIZE))) hm_recordcache_layout_t;

// A hashmap for verifying VCALL signatures
#define VERIFY_GROUP_NUM 81 // 81 groups for verification, ~8 pages
typedef struct              // Inherits from hm_cache_layout_t
{
    hm_map_t hashmap;                                                     // The hashmap instance
    hm_group_t map_groups[VERIFY_GROUP_NUM] __attribute__((aligned(32))); // Adjacent group array
} __attribute__((aligned(PAGE_SIZE))) hm_verifycache_layout_t;

#endif // d0ebdb30_7057_4381_8bec_14222d7952c4

#ifndef a723f5ec_ab7b_47ee_9ef7_c78895504a9e
#define a723f5ec_ab7b_47ee_9ef7_c78895504a9e
// Include guard to prevent multiple inclusions
#include <assert.h>
#include <stdalign.h>
#include <stdlib.h>
#include <sys/mman.h>
#include <unistd.h>

static __always_inline size_t hash_kvpair(hm_keyv_t vcall_sign)
{
    // This simple hash function works because class_id is already a randomized value;
    return (vcall_sign.class_id ^ (size_t)(vcall_sign.vptr)) & ~0x80ul; // discard the 8th bit
}

static __always_inline bool kvpair_equals(hm_keyv_t kv1, hm_keyv_t kv2)
{
    return (kv1.class_id == kv2.class_id) && (kv1.vptr == kv2.vptr);
}

#define HASHFUNC hash_kvpair
#define COMPFUNC kvpair_equals

//-----------------------Begin: Define global variables-----------------------------------
// static hm_recordcache_layout_t record_cache __attribute__((aligned(PAGE_SIZE))) = {
//     .hashmap = {
//         .metainfo = {.cache_type = HM_TYPE_RECORD, 0, 0, MAP_MIGRATE_MIN_FREQ + 1},
//         .items = 0,
//         .size = RECORD_GROUP_NUM * HM_GROUP_SIZE,
//         .n_groups = RECORD_GROUP_NUM,
//         .sentinel = 0,
//     },
//     .map_groups = {[0 ...(RECORD_GROUP_NUM - 1)] = {._ctrl = {HM_EMPTY8B, HM_EMPTY8B}}},
// };

// Force the instance to be page-aligned
// static hm_verifycache_layout_t verify_cache __attribute__((aligned(PAGE_SIZE))) = {
//     .hashmap = {
//         .metainfo = {.cache_type = HM_TYPE_VERIFY, 1, 0, 0},
//         .items = 0,
//         .size = VERIFY_GROUP_NUM * HM_GROUP_SIZE,
//         .n_groups = VERIFY_GROUP_NUM,
//         .sentinel = 0,
//     },
//     .map_groups = {[0 ...(VERIFY_GROUP_NUM - 1)] = {._ctrl = {HM_EMPTY8B, HM_EMPTY8B}}},
// };
// Include the auto-generated static variable definitions
#include "cache_init.inc"
//-------------------------End: Define global variables-----------------------------------

static inline hm_control_t zero_lowest_n_bytes(hm_control_t _ctrl, hm_metadata_t n) __attribute__((always_inline));
static inline int hm_pos(size_t hash) __attribute__((always_inline));
static inline hm_metadata_t hm_meta(size_t hash) __attribute__((always_inline));
static inline hm_metadata_t hm_group_pos(int idx) __attribute__((always_inline));
static inline int hm_idx(int group, hm_metadata_t group_pos) __attribute__((always_inline));
static inline hm_hash_t hm_hash(hm_map_t *map, hm_keyv_t keyv) __attribute__((always_inline));
static inline bool hm_should_reduce(hm_map_t *map) __attribute__((always_inline));
static inline uint16_t hm_match_full(hm_map_t *map, int group) __attribute__((always_inline));
static inline int hm_group(int idx) __attribute__((always_inline));
static inline int hm_sentinel_group(hm_map_t *map) __attribute__((always_inline));
static inline int hm_last_group(hm_map_t *map) __attribute__((always_inline));

static inline uint16_t _hm_probe(hm_metadata_t meta, hm_control_t _ctrl) __attribute__((always_inline));
static inline uint16_t _hm_probe_from(hm_metadata_t group_pos, hm_metadata_t meta, hm_control_t _ctrl) __attribute__((always_inline));
static inline bool _hm_match_metadata(hm_map_t *map, hm_metadata_t meta, int group, int *match_idx) __attribute__((always_inline));
static inline bool _hm_match_metadata_from(hm_map_t *map, hm_metadata_t meta, int group, hm_metadata_t group_pos, int *match_idx) __attribute__((always_inline));
static inline hm_keyv_t *_hm_find_hash(hm_map_t *map, hm_hash_t *hash, hm_keyv_t keyv, int group, hm_metadata_t group_pos) __attribute__((always_inline));
static inline void _hm_insert_at(hm_map_t *map, int group, hm_metadata_t group_pos, hm_hash_t hash, hm_keyv_t keyv) __attribute__((always_inline));

// Interfaces for external manipulation
// static hm_map_t *hm_create(size_t n_groups, hm_usage_t type);
// static void hm_destroy(hm_map_t *map);
// static void hm_remove(hm_map_t *map, hm_keyv_t keyv);

static __always_inline hm_keyv_t *hm_find(hm_map_t *map, hm_keyv_t keyv);
static void hm_insert(hm_map_t *map, hm_keyv_t keyv, hm_data_t data);
static void hm_clear(hm_map_t *map);
static bool hm_iterate(hm_map_t *map, int *idx, hm_keyv_t **key_ref);
static bool transfer_high_freq_entries(hm_map_t *dest_map, hm_map_t *src_map, int freq);

alignas(32) static const hm_metadata_t mask[] = {
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1};

static hm_control_t zero_lowest_n_bytes(hm_control_t _ctrl, hm_metadata_t n)
{
    hm_control_t _m = _mm_loadu_si128((hm_control_t *)&mask[16 - n]);
    return _mm_and_si128(_ctrl, _m);
}

static int hm_pos(size_t hash)
{
    // Use the upper 31 bits for position
    return (int)(hash >> 33);
}

static hm_metadata_t hm_meta(size_t hash)
{
    // Use the lower 7 bits for metadata
    return hash & 0x7f;
}

static int hm_idx(int group, hm_metadata_t group_pos)
{
    return (group * HM_GROUP_SIZE) + group_pos;
}

static int hm_group(int idx)
{
    return idx / HM_GROUP_SIZE;
}

static hm_metadata_t hm_group_pos(int idx)
{
    return idx % HM_GROUP_SIZE;
}

static int hm_sentinel_group(hm_map_t *map)
{
    return hm_group(map->sentinel) + 1;
}

static int hm_last_group(hm_map_t *map)
{
    return hm_group(map->size - 1) + 1;
}

static hm_hash_t hm_hash(hm_map_t *map, hm_keyv_t key)
{
    hm_hash_t hash;
    size_t h = HASHFUNC(key);
    hash.pos = hm_pos(h);
    hash.meta = hm_meta(h);
    return hash;
}

static bool hm_should_reduce(hm_map_t *map)
{
    return map->items >= (HM_LOAD_FACTOR * map->size);
}

static uint16_t hm_match_full(hm_map_t *map, int group)
{
    return ~(_mm_movemask_epi8(map->groups[group]._ctrl));
}

static uint16_t _hm_probe(hm_metadata_t meta, hm_control_t _ctrl)
{
    hm_control_t _match = _mm_set1_epi8(meta);
    return _mm_movemask_epi8(_mm_cmpeq_epi8(_match, _ctrl));
}

static uint16_t _hm_probe_from(hm_metadata_t group_pos, hm_metadata_t meta, hm_control_t _ctrl)
{
    hm_control_t _match = _mm_set1_epi8(meta);
    return _mm_movemask_epi8(
        zero_lowest_n_bytes(
            _mm_cmpeq_epi8(_match, _ctrl), group_pos));
}

static bool _hm_match_metadata(hm_map_t *map, hm_metadata_t meta, int group, int *match_idx)
{
    hm_metadata_t match_group_pos = _tzcnt_u32(_hm_probe(
        meta, map->groups[group]._ctrl));

    *match_idx = hm_idx(group, match_group_pos);
    return (match_group_pos < 32) ? true : false;
}

static bool _hm_match_metadata_from(hm_map_t *map, hm_metadata_t meta, int group,
                                    hm_metadata_t group_pos, int *match_idx)
{
    hm_metadata_t match_group_pos = _tzcnt_u32(_hm_probe_from(
        group_pos, meta, map->groups[group]._ctrl));

    *match_idx = hm_idx(group, match_group_pos);
    return (match_group_pos < 32) ? true : false;
}

// Return the hm_keyv_t* for the key-value pair in the hashmap.
// If the key is not found, return NULL.
// keyv = _hm_find_hash(map, keyv, ...)
static hm_keyv_t *_hm_find_hash(hm_map_t *map, hm_hash_t *hash, hm_keyv_t kv, int group, hm_metadata_t group_pos)
{
    uint16_t matches = _hm_probe_from(group_pos, hash->meta, map->groups[group]._ctrl);

    while (matches)
    {
        hm_metadata_t match_group_pos = _tzcnt_u32(matches);

        if (COMPFUNC(map->groups[group].keyv[match_group_pos], kv))
        {
            return &map->groups[group].keyv[match_group_pos];
        }

        matches = _blsr_u32(matches);
    }
    // If we reach here, we didn't find the key in the current group.
    int match_idx;
    if (_hm_match_metadata_from(map, HM_EMPTY1B, group, group_pos, &match_idx))
        return NULL;

    int end_group = hm_sentinel_group(map);
    while (true)
    {
        group = (group + 1) % end_group;

        matches = _hm_probe(hash->meta, map->groups[group]._ctrl);

        while (matches)
        {
            hm_metadata_t match_group_pos = _tzcnt_u32(matches);

            if (COMPFUNC(map->groups[group].keyv[match_group_pos], kv))
            {
                return &map->groups[group].keyv[match_group_pos];
            }

            matches = _blsr_u32(matches);
        }
        if (_hm_match_metadata(map, HM_EMPTY1B, group, &match_idx))
            return NULL;
    }
}

/**----------------------------------------------------------------------
 *  Interfaces for external manipulation
 ----------------------------------------------------------------------*/
// static hm_map_t *hm_create(size_t n_groups, hm_usage_t type)
// {
//     assert(false && "hm_create() is not supported in this implementation.");
//     return NULL;
// }

// static void hm_destroy(hm_map_t *map)
// {
//     assert(false && "hm_destroy() is not supported in this implementation.");
// }

// void hm_remove(hm_map_t *map, hm_keyv_t keyv)
// {
//     assert(false && "hm_remove() is not supported in this implementation.");
// }

// Return the element pointer of the key-value pair in the hashmap.
// If the key is not found, return NULL.
static hm_keyv_t *hm_find(hm_map_t *map, hm_keyv_t keyv)
{
    hm_hash_t hash = hm_hash(map, keyv);
    int idx = hash.pos % map->size;
    int group = hm_group(idx);
    hm_metadata_t group_pos = hm_group_pos(idx);
    return _hm_find_hash(map, &hash, keyv, group, group_pos);
}

void hm_clear(hm_map_t *map)
{
    hm_control_t _empty = _mm_set1_epi8(HM_EMPTY1B);
    int end_group = hm_sentinel_group(map);

    for (int group = 0; group < end_group; group++)
        map->groups[group]._ctrl = _empty;
    map->items = 0;
    map->sentinel = 0;
}

// Use the FIFO policy to evict the oldest generation from the verify_cache.
bool _hm_reduce_verify(hm_map_t *map)
{
    if (map->n_groups > 0)
    {
        hm_metadata_t group_pos;
        int group, end_group;
        uint16_t match_full;
        int old_gen = map->metainfo.oldest_generation;

        end_group = hm_sentinel_group(map);
        for (group = 0; group < end_group; group++)
        {
            match_full = hm_match_full(map, group);

            while (match_full)
            {
                group_pos = _tzcnt_u32(match_full);

                if (map->groups[group].keyv[group_pos].data <= old_gen)
                {
                    ((hm_metadata_t *)&(map->groups[group]._ctrl))[group_pos] = HM_DELETED;
                    map->items--;
                }

                match_full = _blsr_u32(match_full);
            }
        }
        map->metainfo.oldest_generation++;
        assert(map->metainfo.oldest_generation <= map->metainfo.newest_generation &&
               "oldest_generation should not exceed newest_generation");

        if (map->items == 0)
            map->sentinel = 0;
    }
    return true;
}

// Use the frequency policy to evict entries in the record_cache.
bool _hm_reduce_record(hm_map_t *map)
{
    if (map->n_groups > 0)
    {
        hm_metadata_t group_pos;
        int group, end_group;
        uint16_t match_full;
        int num_evicted = 0; // Number of entries evicted during the reduction

        int min_freq = map->metainfo.eviction_min_freq;
        while (num_evicted <= MAP_EVICT_MIN_COUNT) // Ensure at least N entries are evicted
        {
            end_group = hm_sentinel_group(map);
            for (group = 0; group < end_group; group++)
            {
                match_full = hm_match_full(map, group);

                while (match_full)
                {
                    group_pos = _tzcnt_u32(match_full);

                    if (map->groups[group].keyv[group_pos].data <= min_freq)
                    {
                        ((hm_metadata_t *)&(map->groups[group]._ctrl))[group_pos] = HM_DELETED;
                        num_evicted++;
                        map->items--;
                    }

                    match_full = _blsr_u32(match_full);
                }
            }
            min_freq *= 2;
        }
        if (map->items == 0)
            map->sentinel = 0;
    }

    return true;
}

static void _hm_insert_at(hm_map_t *map, int group, hm_metadata_t group_pos, hm_hash_t hash, hm_keyv_t keyv)
{
    int match_idx, match_idx_emp, match_idx_del;

    bool res1 = _hm_match_metadata_from(map, HM_EMPTY1B, group, group_pos, &match_idx_emp);
    bool res2 = _hm_match_metadata_from(map, HM_DELETED, group, group_pos, &match_idx_del);
    if (res1 || res2)
    {
        match_idx = (match_idx_emp < match_idx_del) ? match_idx_emp : match_idx_del;
        group_pos = hm_group_pos(match_idx);

        ((hm_metadata_t *)&(map->groups[group]._ctrl))[group_pos] = hash.meta;
        map->groups[group].keyv[group_pos] = keyv;
        map->groups[group].hash[group_pos] = hash;

        map->items++;

        if (match_idx > map->sentinel)
            map->sentinel = match_idx;
        return;
    }

    int end_group = hm_last_group(map);

    while (true)
    {
        group = (group + 1) % end_group;

        res1 = _hm_match_metadata(map, HM_EMPTY1B, group, &match_idx_emp);
        res2 = _hm_match_metadata(map, HM_DELETED, group, &match_idx_del);
        if (res1 || res2)
        {
            match_idx = (match_idx_emp < match_idx_del) ? match_idx_emp : match_idx_del;
            group_pos = hm_group_pos(match_idx);

            ((hm_metadata_t *)&(map->groups[group]._ctrl))[group_pos] = hash.meta;
            map->groups[group].hash[group_pos] = hash;
            map->groups[group].keyv[group_pos] = keyv;

            map->items++;

            if (match_idx > map->sentinel)
                map->sentinel = match_idx;
            return;
        }
    }
}

// map_ref[hash(keyv)] = keyv;
// Insert a key-value pair into the hashmap.
void hm_insert(hm_map_t *map_ref, hm_keyv_t keyv, hm_data_t value)
{
    if (hm_should_reduce(map_ref))
    {
        assert(map_ref->size > 0 && "hm_map_t::size must be greater than 0");
        if (map_ref->metainfo.cache_type == HM_TYPE_VERIFY)
            _hm_reduce_verify(map_ref);
        else // assert (map_ref->meta.cache_type == HM_TYPE_RECORD);
            _hm_reduce_record(map_ref);
    }

    hm_hash_t hash = hm_hash(map_ref, keyv);
    int idx = hash.pos % map_ref->size;
    int group = hm_group(idx);
    hm_metadata_t group_pos = hm_group_pos(idx);

    if (map_ref->metainfo.cache_type == HM_TYPE_VERIFY)
        keyv.data = map_ref->metainfo.newest_generation;
    else
        keyv.data = value;

    _hm_insert_at(map_ref, group, group_pos, hash, keyv);
}

// Iterate through the hashmap, idx is the current index in the hashmap.
// *key_ref = map->groups[idx/GROUP_SIZE].keyv[idx%GROUP_SIZE];
// *value_ref = map->data_arr[idx];
bool hm_iterate(hm_map_t *map, int *idx, hm_keyv_t **key_ref)
{
    if (*idx > map->sentinel)
        return false;

    int group = hm_group(*idx);
    hm_metadata_t group_pos = hm_group_pos(*idx);

    *key_ref = NULL;
    if (((hm_metadata_t *)&(map->groups[group]._ctrl))[group_pos] >= 0)
        *key_ref = &map->groups[group].keyv[group_pos];

    (*idx)++;
    return true;
}

//-----------------Begin: Functions for VCFI verification---------------------------------
#define CACHE_MISS_THRESHOLD 100 // Trigger migration when this many entries are recorded

static bool track_vcall_signature(hm_map_t *map_ref, hm_keyv_t keyv)
{
    assert(map_ref->metainfo.cache_type == HM_TYPE_RECORD && "hm_map_t::type must be HM_TYPE_MOREDATA");
    hm_keyv_t *kv = hm_find(map_ref, keyv);
    if (kv)
    {
        kv->data++;
        // Return true if this entry is high frequency
        return kv->data > (CACHE_MISS_THRESHOLD / 5);
    }
    else
    {
        hm_insert(map_ref, keyv, 1);
        return false;
    }
}

// Transfer high frequency entries from src_map to dest_map
static bool transfer_high_freq_entries(hm_map_t *verify_map, hm_map_t *record_map, int freq)
{
    int idx = 0;
    hm_keyv_t *key_ref;

    // Make the verify_map writable during migration
    mprotect(verify_map, sizeof(verify_cache), PROT_READ | PROT_WRITE);
    verify_map->metainfo.newest_generation++;
    while (hm_iterate(record_map, &idx, &key_ref))
    {
        if (key_ref == NULL)
            continue; // Skip empty slots
        if (key_ref->data <= freq)
            continue; // Skip empty values
        hm_insert(verify_map, *key_ref, key_ref->data);
    }
    mprotect(verify_map, sizeof(verify_cache), PROT_READ);
    return true;
}

// Add all VCALL signatures from recording map to validating map
static void migrate_vcall_signature(hm_map_t *verify_map, hm_map_t *record_map)
{
    assert(verify_map->metainfo.cache_type == HM_TYPE_VERIFY && "verify_map must be of type HM_TYPE_ONLYKEYV");
    assert(record_map->metainfo.cache_type == HM_TYPE_RECORD && "record_map must be of type HM_TYPE_MOREDATA");

    transfer_high_freq_entries(verify_map, record_map, MAP_MIGRATE_MIN_FREQ);
    // Clear the record_map after migration
    hm_clear(record_map);
}
//------------------End: Functions for VCFI verification----------------------------------
#endif // a723f5ec_ab7b_47ee_9ef7_c78895504a9e

static volatile bool g_cache_enabled = true;
static volatile bool g_record_cache_lock = false; // false means unlocked
static int g_cache_miss_counter = 0;

extern "C" void __cfi_slowpath_orig(uint64_t CallSiteTypeId, void *Ptr);
// Define boundaries of the VTables, use _etext and _edata as they are defined in your linker script.
extern "C" char __executable_start[]; // __vtable_rodata_start
extern "C" char _etext[]; // __vtable_rodata_end

/**
 * Checks if the vcall signature (type_id, vptr) exists in the verification
 * cache. If not found, inserts it into the record cache and may trigger
 * migration of high-frequency entries.
 *
 * @param type_id The type identifier for the vcall class.
 * @param vptr The virtual pointer value for the vtable pointer.
 */
extern "C" void __cfi_slowpath(uint64_t TypeId, void *Ptr)
{
    // If cache is temporarily disabled for migration, fallback to original slowpath.
    if (!g_cache_enabled)
    { // Reading the non-atomic, volatile bool
        __cfi_slowpath_orig(TypeId, Ptr);
        return;
    }

    // VCall signature to check in the verification cache.
    hm_keyv_t vcall_signature = {.class_id = TypeId, .vptr = (int)(long)Ptr};

    // Verify with the cache table.
    hm_keyv_t *sign = hm_find(&verify_cache.hashmap, vcall_signature);

    // On cache hit, the call is considered valid. Return immediately.
    if (sign)
        return;

    // --- Cache Miss ---
    g_cache_miss_counter++;

    // Attempt to acquire the lock to access the recording table (non-blocking).
    // test_and_set returns the *previous* value. If it was false (unlocked), the
    // condition is true and we enter the critical section.
    if (!__atomic_test_and_set(&g_record_cache_lock, __ATOMIC_ACQUIRE))
    {
        // --- Lock Acquired ---
        bool hot_miss = track_vcall_signature(&record_cache.hashmap, vcall_signature);

        if (hot_miss || (g_cache_miss_counter > CACHE_MISS_THRESHOLD))
        {
            // Disable cache to begin migration. This write is not thread-safe.
            g_cache_enabled = false;

            // Migrate high-frequency signatures from the recording cache to the verification cache.
            migrate_vcall_signature(&verify_cache.hashmap, &record_cache.hashmap);

            // Re-enable cache and reset the counter. This write is not thread-safe.
            g_cache_enabled = true;
            g_cache_miss_counter = 0;
        }

        // Release the lightweight lock.
        __atomic_clear(&g_record_cache_lock, __ATOMIC_RELEASE);
    }

    // Fallback to the original slow path for this VCall.
    __cfi_slowpath_orig(TypeId, Ptr);
}
